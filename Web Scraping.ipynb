{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e302fb39",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. \n",
    "\n",
    "It involves retrieving the HTML content of a webpage, parsing the HTML to locate specific elements or patterns, and extracting the desired data.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.quintersol.com%2Fwp-content%2Fuploads%2F2020%2F05%2Fhow_does_web_scraping_work.png&f=1&nofb=1&ipt=3fed2e7db03120b72bb4632effd2309a3d6d7f70f875063498f0ee78b791114a&ipo=images\">\n",
    "\n",
    "\n",
    "\n",
    "> Important: Please be aware that the following techniques may be illegal when used on websites that prohibit web scraping.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a9f6e",
   "metadata": {},
   "source": [
    "# Can you scrape from all the websites?\n",
    "\n",
    "* Scraping makes the website traffic spike and may cause the breakdown of the website server. Thus, not all websites allow people to scrape. \n",
    "\n",
    "* How do you know which websites are allowed or not? \n",
    "\n",
    "* You can look at the ‘robots.txt’ file of the website. \n",
    "\n",
    "\n",
    "> **Try Google.com/robots.txt**\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*pe7HHTIwhbqJJcEfsYcfaA.png\">\n",
    "\n",
    "\n",
    "You can see that Google does not allow web scraping for many of its sub-websites. However, it allows certain paths like ‘/m/finance’ and thus if you want to collect information on finance then this is a completely legal place to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287550e3",
   "metadata": {},
   "source": [
    "## Step 1: Install necessary libraries\n",
    "Ensure you have Python installed on your system. Additionally, you need to install the BeautifulSoup and requests libraries. Open your command prompt or terminal and run the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9f1fb",
   "metadata": {},
   "source": [
    "## Step 2: Import Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48df850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcecce",
   "metadata": {},
   "source": [
    "## Step 3: GET Request\n",
    "\n",
    "To start web scraping, you need to send a GET request to the desired webpage using the requests library. This fetches the HTML content of the webpage. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3144ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.example.com'  # Replace with the desired webpage URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60466f3",
   "metadata": {},
   "source": [
    "## Step 4: Parse the HTML content\n",
    "With the HTML content fetched, you need to parse it using BeautifulSoup. This library makes it easy to extract data from HTML. Here's an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74cf177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser') # Instead of html.parser you can use \"lxml\" too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962a1f5",
   "metadata": {},
   "source": [
    "## Step 5: Find elements using CSS selectors\n",
    "Using BeautifulSoup, you can find specific elements on the webpage using CSS selectors. For example, to find all the links on the webpage, use the find_all method with the appropriate CSS selector. \n",
    "\n",
    "Open Inspect Page\n",
    "\n",
    "<img src=\"inspect.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853f34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f9d64",
   "metadata": {},
   "source": [
    "## Step 6: Extract data from elements\n",
    "Once you have found the desired elements, extract the data from them. You can access attributes like href or text to get the relevant information. Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc13a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link: More information...\n",
      "URL: https://www.iana.org/domains/example\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    href = link['href']\n",
    "    text = link.text\n",
    "    print(f'Link: {text}\\nURL: {href}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516c286",
   "metadata": {},
   "source": [
    "## Step 7: Handling pagination or multiple pages\n",
    "If the data you want to scrape is spread across multiple pages or requires interaction, you need to handle pagination or navigate through different pages. This involves sending subsequent requests and parsing the HTML content of each page.\n",
    "\n",
    "## Step 8: Data storage\n",
    "Finally, you may want to store the extracted data for further analysis or usage. You can save it to a file, database, or any other storage medium using appropriate methods based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba469c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ccff9c2",
   "metadata": {},
   "source": [
    "# Wroking Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db10f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47a25d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Dionysus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Dionysus\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "Favorite animal: Leopard \n",
      "\n",
      "Favorite Color: Wine\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39b54de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image\n",
    "import os\n",
    "\n",
    "image_elements = soup.find_all('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9ee0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: dionysus.jpg\n",
      "Saved image: grapes.png\n",
      "All images have been saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Step 1: Send a GET request to the webpage\n",
    "url = 'http://olympus.realpython.org/profiles/dionysus'  # Replace with the desired webpage URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find image elements\n",
    "image_elements = soup.find_all('img')\n",
    "\n",
    "# Step 4: Create the 'images' directory\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Step 5: Download and save the images\n",
    "for image in image_elements:\n",
    "    image_url = image['src']\n",
    "    if image_url.startswith('http'):  # Absolute URL\n",
    "        image_source = image_url\n",
    "    else:  # Relative URL\n",
    "        image_source = urljoin(url, image_url)\n",
    "\n",
    "    image_name = os.path.basename(image_source)\n",
    "    image_path = os.path.join('images', image_name)  # Define the path where the image will be saved\n",
    "\n",
    "    # Sending a GET request to the image URL and saving the content to a file\n",
    "    image_response = requests.get(image_source)\n",
    "    with open(image_path, 'wb') as file:\n",
    "        file.write(image_response.content)\n",
    "\n",
    "    print(f'Saved image: {image_name}')\n",
    "\n",
    "print('All images have been saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029de788",
   "metadata": {},
   "source": [
    "# Try Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4bf8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Create top_items as empty list\n",
    "all_products = []\n",
    "\n",
    "# Extract and store in top_items according to instructions on the left\n",
    "products = soup.select('div.thumbnail')\n",
    "for product in products:\n",
    "    name = product.select('h4 > a')[0].text.strip()\n",
    "    description = product.select('p.description')[0].text.strip()\n",
    "    price = product.select('h4.price')[0].text.strip()\n",
    "    reviews = product.select('div.ratings')[0].text.strip()\n",
    "    image = product.select('img')[0].get('src')\n",
    "\n",
    "    all_products.append({\n",
    "        \"name\": name,\n",
    "        \"description\": description,\n",
    "        \"price\": price,\n",
    "        \"reviews\": reviews,\n",
    "        \"image\": image\n",
    "    })\n",
    "\n",
    "\n",
    "keys = all_products[0].keys()\n",
    "\n",
    "with open('products.csv', 'w', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413c246",
   "metadata": {},
   "source": [
    "\n",
    "Further reading\n",
    "* [Web Scraping Pagination: A Simple Web Scraper Tutorial](https://rayobyte.com/blog/web-scraping-pagination/)\n",
    "* [Web Scraping Python Tutorial: How to Scrape Data from a Website](https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/)\n",
    "* [Beautiful Soup Web Scraping Tutorial in Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "* [Web Scraping for Beginners](https://www.sitepoint.com/web-scraping-for-beginners/)\n",
    "* [Best Two Non-Coding Web Data Scraping Tools You Need in Your Toolkit](https://medium.datadriveninvestor.com/best-two-non-coding-web-data-scraping-tools-you-need-in-your-toolkit-be31c7c8693e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e122b75",
   "metadata": {},
   "source": [
    "# Tools you can use\n",
    "\n",
    "<img src=\"https://www.upwork.com/mc/documents/Install-ParseHub.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba3bfc6",
   "metadata": {},
   "source": [
    "# Some Chrome Extension\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/sJR755-g7jgngpgP2zddmlkwblliOh7cav72X-s9QrwlmqemAaUFLCZJw0J6EgNjXBwiOu0qXC8duFwVz58P0uSQ=w128-h128-e365-rj-sc0x00ffffff\">\n",
    "\n",
    "https://chrome.google.com/webstore/detail/grepsr-web-scraping-tool/hjdijkhlfpeafghibmiabeofkiicdnjm\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "<img src=\"https://static.dataminer.io/prod/img/dm/logo-long-h60.png\"> \n",
    "\n",
    "https://chrome.google.com/webstore/detail/data-scraper-easy-web-scr/nndknepjnldbdbepjfgmncbggmopgden\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/M2K0VbhN1C9reWRv8_65g6q3cPJnhBX9EKsYabhvY7Uu6hPOQyBphx3yrGow_nTsPspCwX69lfJb07Z1bSBbqmKToA=w128-h128-e365-rj-sc0x00ffffff\">\n",
    "\n",
    "https://chrome.google.com/webstore/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398f0c3",
   "metadata": {},
   "source": [
    "https://www.scrapingbee.com/blog/web-scraping-tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51f527",
   "metadata": {},
   "source": [
    "# Web Scraping HTML Tables Without BeautifulSoup or Any Scraping Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15931102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Awards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé[a]</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Georg Solti</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quincy Jones</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alison Krauss[b]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Chick Corea</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank            Artist  Awards\n",
       "0     1        Beyoncé[a]      32\n",
       "1     2       Georg Solti      31\n",
       "2     3      Quincy Jones      28\n",
       "3     4  Alison Krauss[b]      27\n",
       "4     4       Chick Corea      27"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the Pandas library\n",
    "import pandas as pd\n",
    "#scrap 1st table data and store as dataframe name df_award1\n",
    "df_award1 = pd.read_html('https://en.wikipedia.org/wiki/Grammy_Award_records') [0]\n",
    "#view the dataset as pandas dataframe object\n",
    "df_award1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09c18e",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfButBVbwnRkkn5W5E5f1g.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080761",
   "metadata": {},
   "source": [
    "Learn More here:  [Web Scraping HTML Tables Without BeautifulSoup or Any Scraping Tool](https://python.plainenglish.io/web-scraping-html-tables-without-beautifulsoup-or-any-scraping-tool-b660803feca7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489673f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
